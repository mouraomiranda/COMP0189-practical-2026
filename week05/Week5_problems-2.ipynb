{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Ye5YFtvToNe"
   },
   "source": [
    "# COMP0189: Applied Artificial Intelligence\n",
    "## Week 7 (Model Interpretation and Feature selection)\n",
    "\n",
    "\n",
    "## Learning goals ðŸŽ¯\n",
    "1. Learn how to use different strategies for interpreting machine learning models.\n",
    "2. Learn how to properly implement feature selection to avoid leaking information.\n",
    "\n",
    "### Acknowledgements\n",
    "- https://scikit-learn.org/stable/\n",
    "- https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html#id1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sJ7N67e1qRY2",
    "outputId": "924db2fc-8119-43a4-e3fb-bf2df91f1a6a"
   },
   "outputs": [],
   "source": [
    "%pip install scikit-learn==1.7.2 matplotlib==3.10.8 pandas==2.3.3 seaborn==0.13.2 imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zp3mpVv4A9_W"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.cluster import hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jbHJ5b5A9_Y"
   },
   "source": [
    "# Part 1: A common error: leaking information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AldmQ19uA9_Y"
   },
   "source": [
    "We will start with a toy example to illustrate a common mistake when using feature selection. We will create a random dataset with 10.000 features and 100 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MAm-RPVBA9_Z"
   },
   "outputs": [],
   "source": [
    "rnd = np.random.RandomState(seed=0)\n",
    "X = rnd.normal(size=(100, 10000))\n",
    "y = rnd.normal(size=(100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7JPA5RNRA9_Z",
    "outputId": "9d82397b-0c53-4e97-b451-eb9ad4856007"
   },
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RC11ZosYA9_a"
   },
   "source": [
    "We might consider that 10.000 is a very high number of features and that we need to use feature selection. So, let's select the 5% most informative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XT3sN3KeA9_a",
    "outputId": "beb36771-54c5-47e1-9be2-a22587fd9dc3"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, f_regression\n",
    "\n",
    "select = SelectPercentile(score_func=f_regression,\n",
    "                          percentile=5)\n",
    "select.fit(X, y)\n",
    "X_sel = select.transform(X)\n",
    "\n",
    "print(X_sel.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlJXXsIeA9_b"
   },
   "source": [
    "Now we will create a pipeline to pre-process the data and fit a regression model to see if we can predict the random labels from the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KSntGdZ8A9_b",
    "outputId": "b362fbc9-47c3-4b9f-f33e-352c8c255534"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sel, y, random_state=0)\n",
    "pipe = make_pipeline(StandardScaler(), Ridge())\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tas54WLSA9_c"
   },
   "source": [
    "These are great results but how did we get such good results on a random dataset?\n",
    "\n",
    "These results are due to information leaking as the features were selected before spliting the data into train and test splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qk97Om8OA9_c"
   },
   "source": [
    "### Task 1: Implement a correct pipeline to pre-process the data, select the top 5% features and train a regression model to predict th random labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rxqz_phZA9_c",
    "outputId": "21287bea-44cb-4666-a952-a2e976cf5a89"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "pipe = ...\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1pCcvR8DA9_d"
   },
   "source": [
    "These results make more sense from what we would expet with random labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBrEYQUbA9_d"
   },
   "source": [
    "# Part 2: Model interpretation and feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1_HOv7dA9_e"
   },
   "source": [
    "### QSAR Biodegradation Dataset\n",
    "\n",
    "**Source:** [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/254/qsar+biodegradation)\n",
    "\n",
    "**Samples:** 1,055 chemicals (356 ready biodegradable, 699 not ready biodegradable)\n",
    "\n",
    "**Target Variable:** Experimental class (**RB** = ready biodegradable, **NRB** = not ready biodegradable)\n",
    "\n",
    "**Features:** 41 molecular descriptors (e.g., SpMax_L, nHM, F04[C-N], nO, nN) used to classify biodegradability.\n",
    "\n",
    "**Purpose:** Development of Quantitative Structure-Activity Relationship (QSAR) models to predict the biodegradability of chemical compounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "2bbrRMTOToNk",
    "outputId": "02280b12-13f5-4ce3-f4f0-0d1b581bcbf7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"biodegradation.csv\")\n",
    "\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssZKhHxZToNk"
   },
   "source": [
    "Now we identify features X and targets y. The column \"experimental class\" is our target variable (i.e., the variable which we want to predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "id": "Vp3beQbEToNl",
    "outputId": "ff950633-bb49-44e4-c1e5-2c83af9c668e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.replace([\"RB\",\"NRB\"],[1,0], inplace = True) #apply decoding\n",
    "#rename target for better readability\n",
    "df.rename(columns = {\"experimental class\": \"degradable\"}, inplace = True);\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df.drop(columns=[\"degradable\"])  # Exclude non-feature columns\n",
    "y = df[\"degradable\"]  # Target variable (1 = ready biodegradable, 0 = not ready biodegradable)\n",
    "\n",
    "# Display summary statistics\n",
    "X.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "44iBUDR6ToNm",
    "outputId": "2e1ee8be-4015-4f1f-a8a9-6323b60ac4bd"
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgOH32mFToNm"
   },
   "source": [
    "Our target for prediction: degradable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "KEv3tul5ToNm",
    "outputId": "389de2bd-c352-4dde-f199-32df62bc9ca0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display the first few values\n",
    "df[\"degradable\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwAEPiHjFfmI"
   },
   "source": [
    "Handle imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q2zD4tHAEH5K"
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eI8IOtYpEIUX",
    "outputId": "7edf1b6e-0bb1-4f3e-da31-69920ffb4f47"
   },
   "outputs": [],
   "source": [
    "print(\"Before Undersampling, counts of label '1': {}\".format(sum(y == 1)))\n",
    "print(\"Before Undersampling, counts of label '0': {} \\n\".format(sum(y == 0)))\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_res, y_res = rus.fit_resample(X, y)\n",
    "\n",
    "print('After Undersampling, the shape of train_X: {}'.format(X_res.shape))\n",
    "print('After Undersampling, the shape of train_y: {} \\n'.format(y_res.shape))\n",
    "\n",
    "print(\"After Undersampling, counts of label '1': {}\".format(sum(y_res == 1)))\n",
    "print(\"After Undersampling, counts of label '0': {}\".format(sum(y_res == 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtFe0csGToNn"
   },
   "source": [
    "## Exploratory data analysis\n",
    "\n",
    "We now split the sample into a train and a test dataset. Only the train dataset will be used in the following exploratory analysis. This is a way to emulate a real situation where predictions are performed on an unknown target, and we donâ€™t want our analysis and decisions to be biased by our knowledge of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IajsRtoNToNn"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, random_state=42,stratify=y_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgEMsTCKToNo"
   },
   "source": [
    "First, letâ€™s get some insights by looking at the a matrix showing the correlation of all features with each other between them. Only numerical variables will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 685
    },
    "id": "zY0615zHToNo",
    "outputId": "bc13813d-c159-4757-ac0e-5d5b0339aac9"
   },
   "outputs": [],
   "source": [
    "correlation_matrix = X_train.corr()\n",
    "\n",
    "plt.figure(figsize=(8, 7))\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm', center=0, square=True,\n",
    "xticklabels=correlation_matrix.columns, yticklabels=correlation_matrix.columns)\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often easier to see structure in the correlation matrix if we reorder the features using hierarchical clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 744
    },
    "id": "3zp8N5gAbnIq",
    "outputId": "adafd52c-a369-4be0-a2c7-238d42a93db1"
   },
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "correlation_matrix = X_train.corr()\n",
    "\n",
    "# Compute distance matrix using absolute correlation (to consider both positive/negative)\n",
    "distance_matrix = 1 - np.abs(correlation_matrix)\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "linkage = hierarchy.linkage(distance_matrix, method='average')\n",
    "order = hierarchy.dendrogram(linkage, no_plot=True)['leaves']\n",
    "reordered_corr = correlation_matrix.iloc[order, order]\n",
    "\n",
    "# Plot clustered heatmap\n",
    "plt.figure(figsize=(8, 7))\n",
    "sns.heatmap(reordered_corr, cmap='coolwarm', center=0, square=True,\n",
    "xticklabels=reordered_corr.columns, yticklabels=reordered_corr.columns)\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.title('Correlation Matrix (clustered order)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kr3AqplYToNp"
   },
   "source": [
    "Before designing a machine learning pipeline, we should check the type of data that we are dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e_maegsSToNp",
    "outputId": "9a237a91-fcd0-4eb7-e56e-33cc08cb58a9"
   },
   "outputs": [],
   "source": [
    "# Check dataset information\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1ljuZ0oToNp"
   },
   "source": [
    "All features are numerical and unbounded, suggesting we should scale all of them before training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qt-dOJIVToNq"
   },
   "source": [
    "## Task 2: Machine Learning Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jA33s0gKqRY4"
   },
   "source": [
    "### Task 2.1 Implement a **machine learning pipeline** that includes **preprocessing and cross-validation** to optimize the model's hyperparameters.\n",
    "- Use the pipeline with **linear SVM** and **regularized logistic regression with L1 and elastic-net regularization** to predict whether a chemical is **degradable or non-degradable** based on the given features.\n",
    "- Create a table to show the performance of the different models.\n",
    "- Plot the confusion matrix and ROC curve for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8GSF28gHToNq"
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Preprocessing: Standardize numerical features\n",
    "preprocessor = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bCa5wT5BqRY4",
    "outputId": "ace1a31f-a193-4115-c0e6-4ddfe1023363"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def optimise_hyperparameters(model: BaseEstimator, param_grid: dict):\n",
    "    preprocess_and_train = ...\n",
    "\n",
    "    grid_search = ...\n",
    "\n",
    "    # Fit GridSearchCV\n",
    "    return grid_search.fit(X_train, y_train)\n",
    "\n",
    "# defining parameter range\n",
    "cv_svc = optimise_hyperparameters(\n",
    "    LinearSVC(dual=\"auto\", random_state=42),\n",
    "    {'classify__C': [0.1, 1]}\n",
    ")\n",
    "model_svc=cv_svc.best_estimator_\n",
    "\n",
    "cv_lasso = optimise_hyperparameters(\n",
    "    LogisticRegression(\n",
    "        penalty=\"l1\",  # Lasso (L1 regularization)\n",
    "        solver=\"liblinear\",  # Required for L1 penalty\n",
    "        max_iter=100000,\n",
    "    ),\n",
    "    {'classify__C': np.logspace(-3, 3, 10)}\n",
    ")\n",
    "model_Lasso = cv_lasso.best_estimator_\n",
    "\n",
    "cv_en = optimise_hyperparameters(\n",
    "    LogisticRegression(\n",
    "        penalty=\"elasticnet\",\n",
    "        solver=\"saga\",\n",
    "        max_iter=100000,\n",
    "    ),\n",
    "    {'classify__C': np.logspace(-3, 3, 10), \"classify__l1_ratio\": [0.1, 0.5, 0.9]}\n",
    ")\n",
    "model_EN = cv_en.best_estimator_\n",
    "\n",
    "print(\"Done training models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "WH9xFRhKqRY5",
    "outputId": "e5dedee4-a666-44b9-d9f2-7fed9d5036d9"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, balanced_accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def get_metrics(name: str, model: Pipeline, use_proba: bool = False):\n",
    "    # Predictions\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    # Probabilities for AUC computation\n",
    "    y_proba_test = model.predict_proba(X_test)[:, 1] if use_proba else model.decision_function(X_test)\n",
    "\n",
    "    # Compute classification metrics\n",
    "    metrics_test = {\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": balanced_accuracy_score(y_test, y_pred_test),\n",
    "        \"Precision\": precision_score(y_test, y_pred_test),\n",
    "        \"Recall\": recall_score(y_test, y_pred_test),\n",
    "        \"F1-score\": f1_score(y_test, y_pred_test),\n",
    "        \"AUC\": roc_auc_score(y_test, y_proba_test),\n",
    "    }\n",
    "\n",
    "    return metrics_test\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-score\", \"AUC\"])\n",
    "results_df.loc[0] = get_metrics(\"SVC\", model_svc)\n",
    "results_df.loc[1] = get_metrics(\"Logistic Regression (L1)\", model_Lasso)\n",
    "results_df.loc[2] = get_metrics(\"Logistic Regression (ElasticNet)\", model_EN)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "t_8Y_MCbqRY5",
    "outputId": "f6a9303e-f7ce-4304-a264-a8014a6bfabf"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay\n",
    "\n",
    "def plot_evaluation_graphs(models: list[tuple[str, Pipeline]]):\n",
    "    fig, ax = plt.subplots(len(models), 2, figsize=(10, 4 * len(models)))\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        # Confusion Matrix\n",
    "        ax[i][0].set_title(f\"{model[0]} - confusion matrix\")\n",
    "        ConfusionMatrixDisplay.from_estimator(model[1], X_test, y_test, display_labels=[\"degradable\", \"non-degradable\"], ax=ax[i][0], cmap=\"Blues\")\n",
    "\n",
    "        # ROC Curve\n",
    "        ax[i][1].set_title(f\"{model[0]} - ROC curve\")\n",
    "        RocCurveDisplay.from_estimator(model[1], X_test, y_test, ax=ax[i][1])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "models = [\n",
    "    (\"SVC\", model_svc),\n",
    "    (\"LR (L1)\", model_Lasso),\n",
    "    (\"LR (EN)\", model_EN),\n",
    "]\n",
    "plot_evaluation_graphs(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lvV0O5eToNr"
   },
   "source": [
    "### Task 2.2 Plot the models coefficients variability across folds for the linear models (please rank the coefficients to facilitate interpretability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxqBSUy4A9_p"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold, cross_validate\n",
    "\n",
    "def get_coefficients(model: Pipeline):\n",
    "    # Get feature names from the preprocessing pipeline\n",
    "    feature_names = ...\n",
    "\n",
    "    # Define repeated k-fold cross-validation\n",
    "    cv = ...\n",
    "\n",
    "    # Perform cross-validation and store estimators\n",
    "    cv_model = ...\n",
    "\n",
    "    # Extract coefficients from trained models\n",
    "    return pd.DataFrame(\n",
    "        [est[-1].coef_.ravel() for est in cv_model[\"estimator\"]],  # Extracting coefficients correctly\n",
    "        columns=feature_names\n",
    "    )\n",
    "\n",
    "coefficients = [\n",
    "    (\"SVC\", get_coefficients(model_svc)),\n",
    "    (\"LR (L1)\", get_coefficients(model_Lasso)),\n",
    "    (\"LR (EN)\", get_coefficients(model_EN)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UnWVVd4PdWsu",
    "outputId": "72d0546d-9023-4ceb-b092-a2cbbf7f6f91"
   },
   "outputs": [],
   "source": [
    "# Plot coefficient variability with ranked coefficients\n",
    "def plot_coefficients(name: str, coefs: pd.DataFrame):\n",
    "    # Calculate median absolute value for each feature and sort\n",
    "    median_abs = coefs.median().sort_values(ascending=False)\n",
    "    sorted_features = median_abs.index.tolist()\n",
    "\n",
    "    # Reorder coefficients by median absolute value\n",
    "    coefs_sorted = coefs[sorted_features]\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.stripplot(data=coefs_sorted, orient=\"h\", palette=\"dark:k\", alpha=0.5)\n",
    "    sns.boxplot(data=coefs_sorted, orient=\"h\", color=\"cyan\", saturation=0.5)\n",
    "    plt.axvline(x=0, color=\".5\")\n",
    "    plt.xlabel(\"Coefficient\")\n",
    "    plt.suptitle(f\"{name} - Optimal Regularization (Sorted by Feature Importance)\")\n",
    "    plt.subplots_adjust(left=0.3)\n",
    "    plt.show()\n",
    "\n",
    "for i, coefficient in enumerate(coefficients):\n",
    "    plot_coefficients(coefficient[0], coefficient[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwJR4MyGToNr"
   },
   "source": [
    "Discussion: Are the coefficents across the different models similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkTgdVizToNr"
   },
   "source": [
    "### Task 2.3 Plot the permutation feature importance for the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "lOd9aFdcHNva",
    "outputId": "98a0da99-46fa-449b-c736-55813ec67fc3"
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Extract feature names\n",
    "feature_names = X_test.columns if hasattr(X_test, 'columns') else [f\"Feature {i}\" for i in range(X_test.shape[1])]\n",
    "\n",
    "# Compute permutation importance on the final estimator (Lasso Logistic Regression)\n",
    "result_svc = ...\n",
    "\n",
    "result_lasso = ...\n",
    "\n",
    "result_en = ...\n",
    "\n",
    "# Plot feature importances\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.bar(np.arange(0, 41) + 0.25, result_svc.importances_mean, yerr=result_svc.importances_std, width=0.25, label=\"SVC\")\n",
    "ax.bar(np.arange(0, 41) + 0.50, result_lasso.importances_mean, yerr=result_lasso.importances_std, width=0.25, label=\"LASSO\")\n",
    "ax.bar(np.arange(0, 41) + 0.75, result_en.importances_mean, yerr=result_en.importances_std, width=0.25, label=\"ElasticNet\")\n",
    "ax.set_xticks(np.arange(0, 41) + 0.5, feature_names, rotation=45, ha=\"right\")\n",
    "ax.legend()\n",
    "ax.set_ylabel(\"Mean Accuracy Decrease\")\n",
    "ax.set_xlabel(\"Feature\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycu4ELxkToNs"
   },
   "source": [
    "Discussion: Are the feature coefficients simimar to the permutation importance for the different models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoX5Z_1FToNs"
   },
   "source": [
    "### Task 2.4 Implement a similar pipeline for tree-based models and use the pipeline with Random Forest and Gradient Boosting trees to predict the degradability from the other features.\n",
    "- Create a table to show the performance of the different models.\n",
    "- Plot the confusion matrix and ROC curve for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LV-pOg9eIThB",
    "outputId": "a74d83fd-5262-4819-e9c0-60ea1e9596ca"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Random Forest Model\n",
    "rf_model = ...\n",
    "\n",
    "# Fit Random Forest model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Gradient Boosting Model\n",
    "gb_model = ...\n",
    "\n",
    "# Fit Gradient Boosting model\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Done training models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "akXcamUaqRY6",
    "outputId": "a928a32c-74c6-4d17-8877-38ef7c26ac65"
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-score\", \"AUC\"])\n",
    "results_df.loc[0] = get_metrics(\"Random Forest\", rf_model, use_proba=True)\n",
    "results_df.loc[1] = get_metrics(\"Gradient Boosting\", gb_model)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "ZKUKxLzwqRY6",
    "outputId": "8e7e6a4f-f009-49c6-d117-dff1a2751b78"
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\"RF\", rf_model),\n",
    "    (\"GB\", gb_model)\n",
    "]\n",
    "\n",
    "plot_evaluation_graphs(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kKckTHiToNs"
   },
   "source": [
    "### Task 2.5 Plot the feature importance for the different tree-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "yKhZrLtfhAQD",
    "outputId": "7f9a1694-0ea5-4ea0-cb89-5f60a91e0917"
   },
   "outputs": [],
   "source": [
    "def plot_feature_importance(model: Pipeline):\n",
    "    # Access the RandomForestClassifier inside the pipeline\n",
    "    random_forest_classifier = model.steps[-1][1]\n",
    "\n",
    "    # Get feature importances\n",
    "    feature_importances = ...\n",
    "\n",
    "    # Extract feature names from the preprocessor\n",
    "    feature_names = ...\n",
    "\n",
    "    # Create a pandas Series for better visualization\n",
    "    importances_series = pd.Series(feature_importances, index=feature_names)\n",
    "\n",
    "    # Plot feature importances\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    importances_series.sort_values().plot.barh(ax=ax, color=\"forestgreen\", alpha=0.7)\n",
    "    ax.set_title(\"Feature Importance\")\n",
    "    ax.set_xlabel(\"Importance\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_feature_importance(rf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "It6_hjh7Jvxs",
    "outputId": "d728a21a-4abb-42aa-ae21-88178e09386d"
   },
   "outputs": [],
   "source": [
    "plot_feature_importance(gb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7O5LF5NToNs"
   },
   "source": [
    "### Task 2.6 Plot the permutation feature importance for the different tree-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "oFDzDpLFToNs",
    "outputId": "8c9c7ac1-9102-4f6a-9ab6-392b8ef9374c"
   },
   "outputs": [],
   "source": [
    "def plot_feature_importance(model):\n",
    "    # Extract feature names\n",
    "    feature_names = X_test.columns if hasattr(X_test, 'columns') else [f\"Feature {i}\" for i in range(X_test.shape[1])]\n",
    "\n",
    "    # Compute permutation importance on the final estimator (RandomForestClassifier inside the pipeline)\n",
    "    result = ...\n",
    "\n",
    "    # Convert to Pandas Series for easy plotting\n",
    "    rf_importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "    # Plot feature importances with error bars\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    rf_importances.sort_values().plot.barh(yerr=result.importances_std, ax=ax, color=\"forestgreen\", alpha=0.8)\n",
    "    ax.set_title(\"Feature Importances using Permutation - Random Forest Classifier\")\n",
    "    ax.set_xlabel(\"Mean Accuracy Decrease\")\n",
    "    ax.set_ylabel(\"Features\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_feature_importance(rf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "q3gUvWyLH8L0",
    "outputId": "2bb89306-2831-4b2e-f970-a6f6cd4ed810"
   },
   "outputs": [],
   "source": [
    "plot_feature_importance(gb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVTTWpnUToNs"
   },
   "source": [
    "Discussion: Are the feature importance and permutation feature importance similar for the different models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFKo9mhlToNt"
   },
   "source": [
    "### Task 2.7  For the best tree-based model use partial dependence plot to investigate dependence between the target response and each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 995
    },
    "id": "wXO8NBfFQzzL",
    "outputId": "958247f3-5f30-4b1d-c393-4bd806603fab"
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "# Predictions & Probabilities for AUC Computation\n",
    "y_pred_gb = ...\n",
    "y_pred_rf = ...\n",
    "\n",
    "y_proba_gb = ...  # Probabilities for positive class\n",
    "y_proba_rf = ...\n",
    "\n",
    "# Compute classification metrics\n",
    "metrics = {\n",
    "    \"Gradient Boosting\": {\n",
    "        \"Accuracy\": balanced_accuracy_score(y_test, y_pred_gb),\n",
    "        \"AUC\": roc_auc_score(y_test, y_proba_gb),\n",
    "        \"F1-score\": f1_score(y_test, y_pred_gb),\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"Accuracy\": balanced_accuracy_score(y_test, y_pred_rf),\n",
    "        \"AUC\": roc_auc_score(y_test, y_proba_rf),\n",
    "        \"F1-score\": f1_score(y_test, y_pred_rf),\n",
    "    }\n",
    "}\n",
    "\n",
    "# Print performance comparison\n",
    "for model, scores in metrics.items():\n",
    "    print(f\"\\n{model} Performance:\")\n",
    "    for metric, value in scores.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Select the best model (based on AUC)\n",
    "best_model = ...\n",
    "best_model_name = \"Gradient Boosting\" if best_model == gb_model else \"Random Forest\"\n",
    "print(f\"\\nBest Model Selected: {best_model_name}\")\n",
    "\n",
    "# Partial Dependence Plot (for best model)\n",
    "features_to_plot = preprocessor.get_feature_names_out()[:6]  # Plot first 6 features for clarity\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "# code here\n",
    "PartialDependenceDisplay.from_estimator(...)\n",
    "plt.suptitle(f\"Partial Dependence Plot - {best_model_name}\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 806
    },
    "id": "1CrlZkmLr8Ml",
    "outputId": "2d621d27-0d2f-44c4-fec5-0058485a5bb6"
   },
   "outputs": [],
   "source": [
    "# Generate Individual Partial Dependence Plots (IPDP)\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# code here\n",
    "PartialDependenceDisplay.from_estimator(...)\n",
    "\n",
    "plt.suptitle(\"IPDP plots\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nL4jXNqvToNt"
   },
   "source": [
    "## Task 3: Include feature selection within the cross-validation pipeline implemented in Task 1 and try two different feature selection strategies (select k best and recursive feature elimination) with the linear SVM model.\n",
    "- Create a table to show the performance of the different models.\n",
    "- Plot the confusion matrix and ROC curve for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lomytm0kqRY6",
    "outputId": "3d70cfc4-512b-4ad9-df29-88fbbd877978"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "def make_feature_selection_pipline(feature_selection_step):\n",
    "    param_grid = {'linearsvc__C': [0.1, 1,]}\n",
    "\n",
    "    model_svc_select = ...\n",
    "\n",
    "    return GridSearchCV(\n",
    "        estimator=model_svc_select,\n",
    "        param_grid=param_grid,\n",
    "        n_jobs=-1,\n",
    "        error_score=0,\n",
    "        verbose=1,\n",
    "        refit=True,\n",
    "    )\n",
    "\n",
    "kbest_pipeline = make_feature_selection_pipline(\n",
    "    SelectKBest(score_func=f_classif, k=10)\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "kbest_result = kbest_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jUyJZ0X7qRY6",
    "outputId": "d1ac99be-e25b-4198-eaa5-6c47211a2312"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "rfe_pipeline = make_feature_selection_pipline(\n",
    "    RFE(estimator=LogisticRegression(max_iter=5000, solver=\"liblinear\"), n_features_to_select=10),\n",
    ")\n",
    "\n",
    "rfe_result=rfe_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "T8hGEX4aqRY7",
    "outputId": "93f0b868-ccad-4dfa-81d8-1f6401873ca0"
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-score\", \"AUC\"])\n",
    "results_df.loc[0] = get_metrics(\"KBest\", kbest_result.best_estimator_)\n",
    "results_df.loc[1] = get_metrics(\"RFE\", rfe_result.best_estimator_)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "4dci3AbaqRY7",
    "outputId": "bee4cec6-cfce-4fa6-ec8b-1356d9c219d6"
   },
   "outputs": [],
   "source": [
    "plot_evaluation_graphs([\n",
    "    (\"KBest\", kbest_result.best_estimator_),\n",
    "    (\"RFE\", rfe_result.best_estimator_)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAAlj1PRToNt"
   },
   "source": [
    "Discussion: Did the model performance improved with feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-J27zzgToNu"
   },
   "source": [
    "### Task 3.2 Plot the coefficientes variability across folds for the linear model based on the selected features (please rank the coefficients to facilitate interpretability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kpewjP15ToNu",
    "outputId": "a49cb262-7aa7-48e2-fbf2-2f15cf6693da"
   },
   "outputs": [],
   "source": [
    "coefs_kbest = ...\n",
    "plot_coefficients(\"KBest\", coefs_kbest)\n",
    "\n",
    "coefs_rfe = ...\n",
    "plot_coefficients(\"RFE\", coefs_rfe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haLNCVvgToNu"
   },
   "source": [
    "Discussion: Are similar features selected using the different strategies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__CrtGFDwCbT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
